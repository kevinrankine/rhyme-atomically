require 'hdf5'
require 'nn'
require 'rnn'
require 'optim'
require 'cutorch'
require 'cunn'

cmd = torch.CmdLine()
cmd:option('-datafile', './data/data.hdf5', 'data file')

function main()
   opt = cmd:parse(arg)
   local datafile = hdf5.open(opt.datafile, 'r')

   local X = datafile:read("X"):all():long()
   local y = datafile:read("y"):all():long()
   local word_vectors = datafile:read("word_vectors"):all():float()
   
   local index_words = io.open('data/index_words.txt', 'r'):read('*a'):split('\n')

   local perm = torch.randperm(X:size(1)):long()
   X = X:index(1, perm)
   y = y:index(1, perm)

   train_X = X:narrow(1, 1, 768)
   train_y = y:narrow(1, 1, 768)
   
   valid_X = X:narrow(1, 769, 128)
   valid_y = y:narrow(1, 769, 128)
   
   model = neural_network(train_X, train_y, word_vectors, valid_X, valid_y)
   generate_lyrics(model, index_words, 120)
end

function neural_network(X_train, y_train, word_vectors, X_valid, y_valid)
   local N = X_train:size(1)
   
   local bsize = 32
   local lsize = 15 -- 20
   local eta = 1e-0
   local nepochs = 1-

   local nwords = word_vectors:size(1)
   local d_in = 100 -- word_vectors:size(2)
   local d_hid = 400
   
   local model = nn.Sequential()
   
   local LT = nn.LookupTable(nwords, d_in)
   LT.weight:copy(word_vectors)

   model:add(LT)
   model:add(nn.SplitTable(2))
   model:add(nn.Sequencer(nn.Dropout(0.1)))
   model:add(nn.Sequencer(nn.FastLSTM(d_in, d_hid)))
   model:add(nn.Sequencer(nn.Dropout(0.1)))
   model:add(nn.Sequencer(nn.Linear(d_hid, nwords)))
   model:add(nn.Sequencer(nn.LogSoftMax()))

   local criterion = nn.SequencerCriterion(nn.ClassNLLCriterion(), true)

   model:cuda()
   criterion:cuda()

   model:remember('both')


   params, gradParams = model:getParameters()
   for epoch = 1, nepochs do
      model:training()
      
      local total_loss = 0

      for ii = 1, N, bsize do
	 print (ii)
	 for jj = 1, X_train:size(2), lsize do
	    gradParams:zero()
	    
	    local xx = X_train:narrow(1, ii, bsize):narrow(2, jj, lsize):cuda()
	    local yy = y_train:narrow(1, ii, bsize):narrow(2, jj, lsize):split(1, 2)

	    yy = map(function(x) return x:select(2, 1):cuda() end, yy)
	    local out = model:forward(xx)

	    local loss = criterion:forward(out, yy)
	    local grad = criterion:backward(out, yy)

	    model:backward(xx, grad)
	    renorm_grad(10, gradParams)
	    
	    model:updateParameters(eta)
	    model:forget()

	    total_loss = total_loss + loss
	 end
      end

      local train_perp = torch.exp(total_loss / ((X_train:size(1) / bsize) * (X_train:size(2) / lsize)))
      print (string.format("Training perplexity after %d epochs: %.2f", epoch, train_perp))
      validate(model, X_valid, y_valid, bsize)
   end

   return model
end

function validate(model, X_valid, y_valid, bsize)
   model:evaluate()

   local N = X_valid:size(1)
   
   local criterion = nn.SequencerCriterion(nn.ClassNLLCriterion(), true):cuda()
   local loss = 0.0
   
   for ii = 1, N, bsize do
      local xx = X_valid:narrow(1, ii, bsize):cuda()
      local yy = y_valid:narrow(1, ii, bsize):split(1, 2)
      yy = map(function(x) return x:select(2, 1):cuda() end, yy)
      
      out = model:forward(xx)

      loss = loss + criterion:forward(out, yy)
      
      model:forget()
   end

   print (string.format("Validation perplexity is %.2f", torch.exp(loss / (X_valid:size(1) / bsize))))
end

function generate_lyrics(model, words_index, start_index)
   model:forget()
   model:evaluate()
   
   local xx = torch.LongTensor({{start_index}}):cuda()
   
   for ii = 1, 300 do
      local dist = torch.exp(model:forward(xx)[1][1])
      dist = dist:narrow(1, 1, dist:size(1) - 1)
      dist = dist / dist:sum()
      
      local cum_dist = dist:cumsum()

      local r = torch.rand(1)[1]
      local index = 1

      while r > cum_dist[index] do
	 index = index + 1
      end
      
      local word = words_index[index]
      local score = dist[index]
      
      print (score, word)

      xx = torch.LongTensor({{index}}):cuda()
   end
end

function map(func, array)
   local new_array = {}
   
   for i,v in ipairs(array) do
      new_array[i] = func(v)
   end
   
   return new_array
end

function renorm_grad(thresh, gradParams)
   local norm = gradParams:norm()

   if (norm > thresh) then
      gradParams:div(norm / thresh)
   end
   
end

main()

